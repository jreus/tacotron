
-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2019-12-22 13:34:57.649]  Checkpoint path: output/logs-taco001/model.ckpt
[2019-12-22 13:34:57.649]  Loading training data from: ../datasets/LJSpeech-Mini/tacotron/train.txt
[2019-12-22 13:34:57.649]  Using model: tacotron
[2019-12-22 13:34:57.649]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: english_cleaners
  decay_learning_rate: True
  decoder_depth: 256
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 200
  min_level_db: -100
  num_freq: 1025
  num_mels: 80
  outputs_per_step: 5
  postnet_depth: 256
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  sample_rate: 20000
  use_cmudict: False
[2019-12-22 13:34:57.720]  Loaded metadata for 186 examples (0.33 hours)
[2019-12-22 13:35:00.300]  Initialized Tacotron model. Dimensions: 
[2019-12-22 13:35:00.300]    embedding:               256
[2019-12-22 13:35:00.300]    prenet out:              128
[2019-12-22 13:35:00.300]    encoder out:             256
[2019-12-22 13:35:00.300]    attention out:           256
[2019-12-22 13:35:00.300]    concat attn & out:       512
[2019-12-22 13:35:00.300]    decoder cell out:        256
[2019-12-22 13:35:00.300]    decoder out (5 frames):  400
[2019-12-22 13:35:00.300]    decoder out (1 frame):   80
[2019-12-22 13:35:00.300]    postnet out:             256
[2019-12-22 13:35:00.300]    linear out:              1025
[2019-12-22 13:35:11.062]  Starting new training run at commit: None
[2019-12-22 13:35:14.198]  Generated 32 batches of size 32 in 3.135 sec
[2019-12-22 13:35:23.956]  Step 1       [12.892 sec/step, loss=0.90950, avg_loss=0.90950]
[2019-12-22 13:35:33.983]  Step 2       [11.460 sec/step, loss=0.93063, avg_loss=0.92007]
[2019-12-22 13:35:33.983]  Saving checkpoint to: output/logs-taco001/model.ckpt-2
[2019-12-22 13:35:35.916]  Saving audio and alignment...
[2019-12-22 13:35:41.610]  Input: it must be said that it is in no way like the transition type of subiaco,~________________

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2019-12-22 13:40:07.077]  Checkpoint path: output/logs-taco001/model.ckpt
[2019-12-22 13:40:07.077]  Loading training data from: ../datasets/LJSpeech-Mini/tacotron/train.txt
[2019-12-22 13:40:07.077]  Using model: tacotron
[2019-12-22 13:40:07.077]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: english_cleaners
  decay_learning_rate: True
  decoder_depth: 256
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 200
  min_level_db: -100
  num_freq: 1025
  num_mels: 80
  outputs_per_step: 5
  postnet_depth: 256
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  sample_rate: 20000
  use_cmudict: False
[2019-12-22 13:40:07.141]  Loaded metadata for 186 examples (0.33 hours)
[2019-12-22 13:40:09.594]  Initialized Tacotron model. Dimensions: 
[2019-12-22 13:40:09.594]    embedding:               256
[2019-12-22 13:40:09.594]    prenet out:              128
[2019-12-22 13:40:09.594]    encoder out:             256
[2019-12-22 13:40:09.594]    attention out:           256
[2019-12-22 13:40:09.594]    concat attn & out:       512
[2019-12-22 13:40:09.594]    decoder cell out:        256
[2019-12-22 13:40:09.594]    decoder out (5 frames):  400
[2019-12-22 13:40:09.594]    decoder out (1 frame):   80
[2019-12-22 13:40:09.594]    postnet out:             256
[2019-12-22 13:40:09.594]    linear out:              1025
[2019-12-22 13:40:19.986]  Starting new training run at commit: None
[2019-12-22 13:40:22.000]  Generated 32 batches of size 32 in 2.014 sec
[2019-12-22 13:40:34.081]  Step 1       [14.095 sec/step, loss=0.90786, avg_loss=0.90786]
[2019-12-22 13:40:36.566]  Step 2       [8.290 sec/step, loss=0.81578, avg_loss=0.86182]
[2019-12-22 13:40:36.566]  Saving checkpoint to: output/logs-taco001/model.ckpt-2
[2019-12-22 13:40:38.561]  Saving audio and alignment...
[2019-12-22 13:40:42.714]  Input: the roman letter was used side by side with the gothic.~_________
[2019-12-22 13:40:53.470]  Step 3       [9.111 sec/step, loss=0.92230, avg_loss=0.88198]
[2019-12-22 13:40:57.490]  Step 4       [7.838 sec/step, loss=0.90775, avg_loss=0.88842]
[2019-12-22 13:40:57.490]  Saving checkpoint to: output/logs-taco001/model.ckpt-4
[2019-12-22 13:40:59.326]  Saving audio and alignment...
[2019-12-22 13:41:06.095]  Input: and though more roman than that, yet scarcely more like the complete roman type of the earliest printers of rome.~
[2019-12-22 13:41:15.428]  Step 5       [8.137 sec/step, loss=0.90707, avg_loss=0.89215]
[2019-12-22 13:41:15.428]  Writing summary at step: 5
[2019-12-22 13:41:31.970]  Step 6       [7.199 sec/step, loss=0.78613, avg_loss=0.87448]
[2019-12-22 13:41:31.970]  Saving checkpoint to: output/logs-taco001/model.ckpt-6
[2019-12-22 13:41:33.548]  Saving audio and alignment...
[2019-12-22 13:41:40.194]  Input: after the end of the fifteenth century the degradation of printing, especially in germany and italy,~__________
